\documentclass[]{elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
\usepackage[hyphens]{url}

  \journal{Journal of Statistical Software} % Sets Journal name


\usepackage{lineno} % add
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\usepackage{graphicx}
\usepackage{booktabs} % book-quality tables
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{fontspec}
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\bibliographystyle{elsarticle-harv}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Practical Applications for a Distributed Modeling Framework for Protected Data},
            colorlinks=false,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls

\setcounter{secnumdepth}{5}
% Pandoc toggle for numbering sections (defaults to be off)

\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{cslreferences}%
  {\setlength{\parindent}{0pt}%
  \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces}%
  {\par}

% Pandoc header



\begin{document}
\begin{frontmatter}

  \title{Practical Applications for a Distributed Modeling Framework for Protected Data}
    \author[Johns Hopkins Bloomberg School of Public Health]{John Muschelli III\corref{Corresponding Author}}
   \ead{jmusche1@jhu.edu} 
      \address[Johns Hopkins Bloomberg School of Public Health]{Department of Biostatistics, 615 N Wolfe St, Baltimore MD, 21205}
    
  \begin{abstract}
  We present 2 practical approaches to fitting generalized linear models in a distributed framework. The use case for this framework is where multiple sites (e.g.~hospitals) have data with protected or private information that cannot be shared, but aggregate data can be, and each site can send aggregated data across the internet. The first strategy involves using an application programming interface (API), where sites submit the data to this service. The second strategy uses synced folder services, such as Dropbox or Box Sync, to share the aggregated data. We provide an R package of examples and code to create an API on DigitalOcean at https://github.com/muschellij2/distribglm.
  \end{abstract}
  
 \end{frontmatter}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Data from electronic health records (EHRs) provide information on routine clinical care compared to research clinical trials, including the differing data quality and quantity captured. Unlike clinical trials, which have consent for data sharing and data usage agreements, individual patient-level data from EHRs are less likely to have authorization for sharing by default. In many cases, however, aggregate data can be shared for research results, reporting, or collaboration with other institutions as these aggregated data cannot be traced back to individuals. Combining this data from multiple sites allows for analyses that are more statistically powerful, more generalizable, and potentially less-biased due to any one individual site or hospital clinical population demographics.

We introduce a distributed framework and a practical tool to analyze data from multiple sources with the data being properly siloed, sharing only aggregated information. The motivation for this problem is that we would like to fit a generalized linear model (GLM) on data from multiple sites (e.g.~hospitals). The idea is that a model is specified, and instructions or software are sent to each site where a summary statistic is computed and returned to the modeling service/site. The model is then updated and, if necessary, the process is completed until the model converges. The goal is to fit the exact model as if the full, aggregated data set was accessible. We focus on GLMs because they cover a variety of problems and can be fit with low-dimensional aggregated statistics to send to a server, reducing the potential for patient identifiability. We will discuss how these solutions have been presented before, but we present \textbf{practical} tools and software for researchers to achieve these goals without creating these tools from scratch.

The alternatives to this process is meta-analysis, one-step solutions, or remote analysis servers (O'Keefe and Good, 2008). There are drawbacks to a meta-analysis approach in that the model and statistics must be specified and commonly the data is gathered \textbf{once} from each site. Also, if sites present estimates from models with different predictors (e.g.~different estimators), it is unclear how meta-analysis adequately handles this variability. The process below ensures that the same model, with the same predictors, is fit at each site. Any updated analysis requires additional correspondence between the modeler and the site data analyst, which slows down the process of analysis and creates more hurdles.

A series of works have covered this topic. The work presented here is almost identical to Grid Binary LOgistic REgression (GLORE) (Wu et al., 2012), its Bayesian analog EXpectation Propagation LOgistic REgRession (EXPLORER) (Wang et al., 2013), and Secure Pooled Analysis acRoss K-sites (SPARK) (El Emam et al., 2013). A few of the differences are: GLORE and EXPLORER focus primarily on logistic regression, SPARK works for all GLMs like the proposed method. GLORE and SPARK are iterative requiring all sites to provide updates until the next estimate can be achieved, like the proposed method; EXPLORER is iterative but asynchronous so that sites can share updates without coordination. SPARK additionally computes on encrypted data, which allows for higher security; EXPLORER uses random matrix implementations to increase security, but required inter-site communication. Similarly, Wang et al. (2017) show an iterative method for regularized/sparse model fitting. Jordan et al. (2019) present a general framework by sending surrogate likelihood information in an iterative way, which generalizes beyond GLMs, and they provide additional examples of M-estimators, regularized/penalized models, and Bayesian models. Notably, all previous solutions do not provide \textbf{practical steps} or tutorials to set up such a system, however.

To alleviate the need for iterative processes, one-shot solutions have been presented. For example, One-shot Distributed Algorithm to perform Logistic regressions (ODAL) (Duan et al., 2019) is a distributed estimation procedure for a logistic model. For a given \(\beta\) from a model, ODAL computes gradients of a likelihood and use these computed gradients to inform the estimate of \(\beta\), but only with one iteration. Duan et al. (2019) have shown to get close estimates to the ``true model'', which would be the model fit if all data was included. The work is expanded upon in Robust-ODAL (Tong et al., 2020), where median gradients are estimated as opposed to mean gradients, to diminish the influence of heterogeneous site data. Other methods have shown that efficient one-step estimators or averages perform well compared to an ``oracle model'' with the full data (Battey et al., 2018; Zhang et al., 2013).

These one-shot methods provide great approximate solutions because 1) they are not iterative, 2) can be computed in a privacy-preserving way, 3) can be robust to outlying data, and 4) can be seen as likelihood updates. As a likelihood update procedure, the same process can be done with all but one site, determining the robustness of the procedure in a sensitivity analysis. The downsides are that the solution is approximate and if another model is to be run, the whole process has to be repeated. Thus, we believe a remote analysis server can be more general, as it reduces the need for communication with the site and the modeler.

The work presented here is similar to that of O'Keefe and Good (2009) as it has a remote analysis server, but with an important difference. In O'Keefe and Good (2009) and most remote analysis servers, a full dataset is aggregated on the remote server so that a full model can be estimated. That is, the full data is available to that server, but not those submitting the models. We wish to avoid the full dataset ever being created. We will perform computations at each site separately, then combining them, also sometimes referred to as federated learning. Practical implementations of servers for federated learning, such as WebDISCO (web service for distributed Cox model learning) (Lu et al., 2015) exist, but these rely on a third-party service to use. Many researchers and their institutions may not allow a sharing of data with these services and their own server must be created. We aim to provide tools to create this server. (At this time the WebDISCO URL https://webdisco.ucsd-dbmi.org:8443/cox/ was not a working link)

Most recent federated learning methods focus on neural networks. The most common federated learning architecture is Federated Averaging (McMahan et al., 2016), which uses stochastic gradient descent to combine neural network parameters from multiple devices into one platform. Many platforms have been developed for this type of federated learning, such as the Federated Learning platform from Google for deep learning (Bonawitz et al., 2019), Content Object Repository Discovery and Registration/Resolution Architecture (CORDA) (Rehak et al., 2005), yet implementation details are typically left out, as many health systems have different rules and implementations. Geyer et al. (2017) and \url{https://comind.org/} provide code on how to set up clients and run a federated deep learning model, but we wish to focus on simpler GLMs in this work, but acknowledge similar inference could be reframed in work based on Tensorflow, a neural network, or their general gradient descent implementations. For more information, Li et al. (2019) provides an good overview of federated learning methods.

One of the main hurdles is that remote analysis servers may be complicated and costly to set up or have little to no instruction on how to set them up, save for the examples with code above. We will present a system that does have the same constraints, as it will rely on a few scripts or spooling up on a server on a low-cost online service with one command.

We acknowledge we are not experts in distributed/federated computing, but would like to show how this process is possible, overlooking obvious hurdles such as authentication, load balancing, network issues, debugging, and overall security on the server side. That said, some of the previous methods and authors present solutions with a series of additional checks with privacy-preserving measures which can be implemented. We argue that analysts already have this responsibility of security, but in a more informal way. In some of the applications above, such as some of the one-shot solutions, the model specification and updates are communicated by easy, likely non-secure methods such as email. Thus, it would seem as though iterative methods can give exact solutions in a secure way, but the iterative process is burdensome, and one-shot solutions are user-friendly but potentially as insecure as the proposed method. We wish to make the iterative process more accessible and user-friendly.

A large reason for the importance of usability is that the statisticians and data analysts that will be running the models will likely be using a statistical language, usually either R or Python. Though we will focus on R, the ideas here can be extended to other languages and systems (R Core Team, 2020). We implemented a practical solution in an R package that allows researchers to practically implement this system with real data. The solution can be done a number of ways; we implemented 1) code to deploy an API (application programming interface) on a remote server and 2) scripts to calculate the model if using a synced folder, backed by services such as Dropbox, Box Sync, or OneDrive. These practical solutions solve the motivating problem, allowing us to fit many different types of models with little technical overhead while keeping protected health information (PHI) private. All code required to set up this platform is open-source and is located in the \texttt{distribglm} package: https://github.com/muschellij2/distribglm. We have an example server as well: \url{https://rsconnect.biostat.jhsph.edu/distribglm/}.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

We will describe generally the example of estimating a GLM, the sufficient statistics needed to fit the model, and how the data is aggregated to fit this model. For each step, we will note the corresponding function or code that performs these steps. These technical aspects can be skipped, but are crucial on describing how the statistics and values sent to the server are aggregate and not individual-level.

\hypertarget{motivating-example}{%
\subsection{Motivating Example}\label{motivating-example}}

We wish to estimate a GLM an outcome \(Y\) on a set of covariates \(X\), with a link function \(g\). Let there be \(K\) hospitals, and \(\mathbf{Y}\) and \(\mathbf{X}\) are on the data on all hospitals \(1, \dots, K\). The model is then specified as:

\[
g(E[\mathbf{Y} | \mathbf{X}]) = \mathbf{X}\beta = \eta
\]
where \(\eta\) is the linear combination of predictors and coefficients. Let us also say that we are interested in \(p\) covariates, and \(n_{k}\) is the total number of records at hospital \(k\) and \(n = \sum\limits_{1}^{k} n_{k}\), is the number of rows of \(\mathbf{Y}\) and \(\mathbf{X}\), thus \(\mathbf{Y}\) is an \(n \times 1\) vector and \(\mathbf{X}\) is a \(n \times p\) matrix. Note, \(p\) is fixed across sites (all covariates are the same). To estimate \(\beta\), we would use:

\[
(\mathbf{X}'W\mathbf{X})^{-1} \mathbf{X}'W\mathbf{Y}
\]
where \(W\) is a matrix of weights determined by \(g\). Since we don't have access to the full \(\mathbf{X}\), but rather a series of \(X_{k}\), \(k = 1, \dots, K\), we could use a number of optimization techniques to find minimums/maximums such as parallelized gradient descent approaches (Mcdonald et al., 2009; Zinkevich et al., 2010) or approximate maximum-likelihood approaches (Duncan, 1980). We will instead use the Fisher scoring method outlined in McCulloch (2000) (page 42), which is commonly used in basic statistical software as it is easily implemented and converges quickly in many cases. The method is performed such that we get subject-level residuals \(u_{i,j,k}\), where \(i\) is the subject and \(j\) denotes the covariate index for covariate \(x_{j}\) where \(j = 1\dots p\) and \(k\) is again site:

\[
u_{i,j, k} = W (y_{i,k} - \mu) \frac{d\eta}{d\mu}x_{i,j,k}
\]
where we will get \(u_{j, k} = \sum_{i} u_{i, j, k}\). Stacking all the \(u_{j,k}\) together will give us the site-aggregated gradient vector \(u_{k}\) for all covariates. Let \(V\) be the variance function determined the GLM, evaluated at \(\mu\), and \(W\) is defined by:

\[
W^{-1} = \left(\frac{d\eta}{d\mu}\right)^2V
\]
which is site-agnostic. If we let \(A_{k} = X_k'WX_k\) we can get \(A = \sum_{k}A_{k}\), which is a \(p \times p\) matrix. Combining this with the \(u_{k}\) provided by each site, which is a \(p\times 1\) vector, then we can calculate \(u = \sum_{k} u_{k}\) to get the necessary gradient \(\nabla\beta\) by \(A^{-1} u\). To estimate the dispersion parameter (\(\phi\)), we need the sample size for each site \(n_{k}\) and the sum of squared residuals, which each are a scalar number.

Another way of thinking of this process is that in standard GLM fitting, each individual contributes to a residual value to the calculation that is aggregated, usually by the sum. The same process is being done here, except that individuals are grouped an aggregated respective to their site, aggregated at the site level, then fully aggregated to give the same estimates as would be given in standard GLM fitting with the full data.

\hypertarget{summary-values-shared}{%
\subsubsection{Summary values shared}\label{summary-values-shared}}

Thus, for generalized linear models, we need only to pass a \(p\times{p}\) matrix \(A\) and \(p\times{1}\) vector \(u\), and 2 scalar values from each site. Additional measures such as the deviance and the log likelihood can be shared to estimate commonly-requested values, such as Akaike information criteria (AIC) (Akaike, 1998). It may be possible to determine individual-level covariates or outcomes based on repeated model fitting and estimating the changes in these values, the current implementation requires the site to \emph{actively} send this data for the new model, and the model form can be seen by each site. Thus, each site would have the ability to not submit data to a model if it believes it would compromise data security.

\hypertarget{implementation}{%
\subsection{Implementation}\label{implementation}}

Thus, one must specify a formula for the model, the exponential family to fit the GLM (e.g.~logistic), and a link function, just as you would any GLM. As the form of the model is the same across sites (i.e.~no additional covariates for site 1 vs.~site 2) this computation relies on sites having the same data structure. In this implementation, they must have the same variable names for the covariates. Without being able to see or clean the full data, each site must ensure this separately. Fortunately, groups such as the Observational Medical Outcomes Partnership (OMOP) have provided a Common Data Model, which has been adopted by programs such as OHDSI (Observational Health Data Sciences and Informatics) to unify EHR data (Hripcsak et al., 2015; Stang et al., 2010). These initiatives provide a framework for data unification required for this implementation to work.

To fit this model, we provide the \texttt{distribglm} (https://github.com/muschellij2/distribglm) R package to perform the distributed learning models. The functions provided wherein allow for the fitting of models in addition to 2 specific sets of examples: a \texttt{plumber} file and a set of examples files using a ``synced folder''.

\hypertarget{synced-folder-implementation}{%
\subsubsection{Synced Folder Implementation}\label{synced-folder-implementation}}

Here we will demonstrate how to fit a model where the sharing of the aggregated statistics is done using a synced folder where all sites would have access to the synced folder. Example synced folders are Dropbox, Box Sync, or Microsoft OneDrive. This folder will only contain information about the model specifications and the estimates above. \textbf{No PHI would go in this folder}, as it is shared across sites.

In many cases, one of the sites contributing data will also be the site specifying the model. We will denote this site the compute site, though this site may does not need to contribute to the model and can be an independent analyst with access to the shared folder.

The overall process is shown, with the respective functions, in Figure \ref{fig:workflow}.

\textbackslash begin\{figure\}
\includegraphics[width=1\linewidth]{workflow} \textbackslash caption\{Illustration of the overall workflow. An analyst sets up the model using the setup\_model function. Once that is done, each respective site reads in their data and then runs estimate\_model, which computes gradients and sends them to the folder/server. Once all gradient values are computed, the coefficients for the next iterations are returned and the gradient is computed again. This process is repeated until convergence or a fixed number of iterations. The final model is then located in the synced folder/server, can be downloaded, and the iterations can either be deleted or investigated to ensure convergence and private information has been secure.\}\label{fig:workflow}
\textbackslash end\{figure\}

The first step in the process is to set up the model (with the \texttt{setup\_model} function), which requires the synced folder location (on the local machine), the generalized linear model formula, which exponential family (and link function) is to be fit, a model name, the identifiers for all the sites to use for this model. The site identifiers are necessary for bookkeeping and works as a way to keep track if all necessary sites have computed the aggregated statistics and the next updated \(\beta\) estimates can be computed.

For example, let's say we have an indicator of death, 3 sites, and want to fit a logistic regression with age and sex as predictors, and we were using Dropbox the code would be:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(distribglm)}
\KeywordTok{setup\_model}\NormalTok{(}\DataTypeTok{model\_name =} \StringTok{"death\_age\_sex"}\NormalTok{, }
            \DataTypeTok{formula =} \StringTok{"death \textasciitilde{} age + sex"}\NormalTok{, }
            \DataTypeTok{family =} \KeywordTok{binomial}\NormalTok{(), }
            \DataTypeTok{all\_site\_names =} \KeywordTok{c}\NormalTok{(}\StringTok{"site1"}\NormalTok{, }\StringTok{"site2"}\NormalTok{, }\StringTok{"site3"}\NormalTok{),}
            \DataTypeTok{synced\_folder =} \StringTok{"\textasciitilde{}/Dropbox/shared\_folder"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once the model is set up, an output file (an R data file) is synced across all sites and the compute site. The data sites then run \texttt{estimate\_model} at the same time and the compute site runs \texttt{compute\_model}.

The \texttt{estimate\_model} function requires the user to specify the the synced folder location, as this local path will vary across sites/computers, the model being fit (as multiple models can be running), the site name, and the data set to be used to estimate the data. This data set \textbf{is not} located in the synced folder. Each site must have independent code to read in the data into an R \texttt{data.frame}.

At each iteration, \texttt{estimate\_model} submits the summary values specified in section \ref{summary-values-shared} as an R data file to the synced folder and then waits for the next iteration. For example, for site 2, the following code could be:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(distribglm)}
\NormalTok{site2\_data =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"/path/to/mortality\_data.csv"}\NormalTok{)}
\KeywordTok{estimate\_model}\NormalTok{(}
  \DataTypeTok{model\_name =} \StringTok{"death\_age\_sex"}\NormalTok{, }
  \DataTypeTok{site\_name =} \StringTok{"site2"}\NormalTok{, }
  \DataTypeTok{data =}\NormalTok{ site2\_data,}
  \DataTypeTok{synced\_folder =} \StringTok{"C:/Dropbox/my\_shared\_folder"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

The \texttt{compute\_model} code looks in the synced folder and determines if the all the sites' data has been submitted and the updated estimate can be computed for the next iteration. Once that is computed, the new estimates are used by the sites in \texttt{estimate\_model} and the process is repeated until convergence. A \texttt{tolerance} argument exists to control the tolerance required to define convergence, which is equal to \(10^{-8}\), the same as defined in the \texttt{glm.control} function in R.

Once the model has converged, the model will be returned and saved. At that point, all iterations could be deleted or archived. Again, this procedure requires all site data to be submitted before a new estimate is produced, so ideally all sites model fitting is run at the same time. We believe a simple conference call walking through each step is a practical example of when this would occur. We present these estimation and computation steps as separate as most cases have a central analysis site and other contributing sites. In the \texttt{estimate\_model} function, if a site sets the argument \texttt{run\_compute} is set to \texttt{TRUE}, then this function will compute the site-level estimates, check to see if all sites have contributed data, and run \texttt{compute\_model}. Thus, after running \texttt{setup\_model}, the only function required for all sites to run is \texttt{estimate\_model}, which will returned the model when the model converges or hits a maximum number of iterations (default 100).

\hypertarget{plumber-implementation}{%
\subsubsection{Plumber Implementation}\label{plumber-implementation}}

In order to make this process more streamlined, one could create an API (Application programming interface). The R package \texttt{plumber} creates APIs with R code (Trestle Technology, LLC, 2018). Though there are many frameworks to create APIs, we choose \texttt{plumber} as it is the most popular framework for R. APIs also require a server. The \texttt{plumberDeploy} package builds on the \texttt{plumber} framework for deploying APIs to a server with only a few commands. RStudio Connect (https://rstudio.com/products/connect/) is also a paid service with the ability to deploy plumber APIs directly from the RStudio software. As this service may be expensive or require extensive expertise to set up, we will show how to set this up using R functions and a cheap server solution.

The \texttt{plumberDeploy} package uses the \texttt{analogsea} package to connect with DigitalOcean (https://www.digitalocean.com/), an easy-to-use cloud deployment service to provide the server (Chamberlain et al., 2020). Many other cloud-based solutions exist, such as Amazon Web Services (AWS), but these deployments are not currently implemented in the \texttt{plumberDeploy} package.

The first step in deploying the API for distributed GLM fitting is to sign up for DigitalOcean. The services are not free and require a form of payment. We will create a virtual machine on DigitalOcean, referred to as a ``droplet'', which we will use as our API server. You will then need to authenticate your DigitalOcean credentials in R. The easiest way to do this is to create a token at https://cloud.digitalocean.com/settings/tokens/new, and set it as an environment variable \texttt{DO\_PAT}. See the \href{https://github.com/sckott/analogsea}{\texttt{analogsea}} package page for more information.

Next, running \texttt{distribglm::do\_deploy\_glm\_api} function will connect to DigitalOcean and create a droplet, and install all the requirements for the API on the droplet, and deploy the API. If this step is successful, you have a working API! This function calls the \texttt{do\_deploy\_glm\_api\_only} function, which deploys the API to the droplet. The \texttt{do\_deploy\_glm\_api\_only} function will allow you to deploy additional APIs, which may be useful if you want different APIs for different projects. Multiple APIs can be deployed on a single droplet. Additional R packages (if required) can be installed via arguments of \texttt{do\_deploy\_glm\_api}. Below, we first create a droplet with \texttt{analogsea} and pass this into the deployment function.

This process may take a few minutes as it is spooling up the server, installing R, a number of system dependencies, and a number of R packages. The \texttt{droplet} object returned contains information about the droplet:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{droplet}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $id
## [1] 210324177
## 
## $name
## [1] "TougherAviation"
## 
## $memory
## [1] 1024
## 
## $vcpus
## [1] 1
## 
## $disk
## [1] 25
## 
## $locked
## [1] FALSE
## 
## $status
## [1] "active"
## 
## $kernel
## NULL
## 
## $created_at
## [1] "2020-10-02T15:56:50Z"
## 
## $features
## $features[[1]]
## [1] "private_networking"
## 
## 
## $backup_ids
## list()
## 
## $next_backup_window
## NULL
## 
## $snapshot_ids
## list()
## 
## $image
## $image$id
## [1] 69439389
## 
## $image$name
## [1] "18.04 (LTS) x64"
## 
## $image$distribution
## [1] "Ubuntu"
## 
## $image$slug
## [1] "ubuntu-18-04-x64"
## 
## $image$public
## [1] TRUE
## 
## $image$regions
## $image$regions[[1]]
## [1] "nyc3"
## 
## $image$regions[[2]]
## [1] "nyc1"
## 
## $image$regions[[3]]
## [1] "sfo1"
## 
## $image$regions[[4]]
## [1] "nyc2"
## 
## $image$regions[[5]]
## [1] "ams2"
## 
## $image$regions[[6]]
## [1] "sgp1"
## 
## $image$regions[[7]]
## [1] "lon1"
## 
## $image$regions[[8]]
## [1] "ams3"
## 
## $image$regions[[9]]
## [1] "fra1"
## 
## $image$regions[[10]]
## [1] "tor1"
## 
## $image$regions[[11]]
## [1] "sfo2"
## 
## $image$regions[[12]]
## [1] "blr1"
## 
## $image$regions[[13]]
## [1] "sfo3"
## 
## 
## $image$created_at
## [1] "2020-09-02T19:36:10Z"
## 
## $image$min_disk_size
## [1] 15
## 
## $image$type
## [1] "base"
## 
## $image$size_gigabytes
## [1] 2.36
## 
## $image$description
## [1] "Ubuntu 18.04 x86 image"
## 
## $image$tags
## list()
## 
## $image$status
## [1] "available"
## 
## 
## $volume_ids
## list()
## 
## $size
## $size$slug
## [1] "s-1vcpu-1gb"
## 
## $size$memory
## [1] 1024
## 
## $size$vcpus
## [1] 1
## 
## $size$disk
## [1] 25
## 
## $size$transfer
## [1] 1
## 
## $size$price_monthly
## [1] 5
## 
## $size$price_hourly
## [1] 0.00744
## 
## $size$regions
## $size$regions[[1]]
## [1] "ams2"
## 
## $size$regions[[2]]
## [1] "ams3"
## 
## $size$regions[[3]]
## [1] "blr1"
## 
## $size$regions[[4]]
## [1] "fra1"
## 
## $size$regions[[5]]
## [1] "lon1"
## 
## $size$regions[[6]]
## [1] "nyc1"
## 
## $size$regions[[7]]
## [1] "nyc2"
## 
## $size$regions[[8]]
## [1] "nyc3"
## 
## $size$regions[[9]]
## [1] "sfo1"
## 
## $size$regions[[10]]
## [1] "sfo2"
## 
## $size$regions[[11]]
## [1] "sfo3"
## 
## $size$regions[[12]]
## [1] "sgp1"
## 
## $size$regions[[13]]
## [1] "tor1"
## 
## 
## $size$available
## [1] TRUE
## 
## 
## $size_slug
## [1] "s-1vcpu-1gb"
## 
## $networks
## $networks$v4
## $networks$v4[[1]]
## $networks$v4[[1]]$ip_address
## [1] "10.120.0.2"
## 
## $networks$v4[[1]]$netmask
## [1] "255.255.240.0"
## 
## $networks$v4[[1]]$gateway
## [1] "<nil>"
## 
## $networks$v4[[1]]$type
## [1] "private"
## 
## 
## $networks$v4[[2]]
## $networks$v4[[2]]$ip_address
## [1] "64.225.124.41"
## 
## $networks$v4[[2]]$netmask
## [1] "255.255.240.0"
## 
## $networks$v4[[2]]$gateway
## [1] "64.225.112.1"
## 
## $networks$v4[[2]]$type
## [1] "public"
## 
## 
## 
## $networks$v6
## list()
## 
## 
## $region
## $region$name
## [1] "San Francisco 2"
## 
## $region$slug
## [1] "sfo2"
## 
## $region$features
## $region$features[[1]]
## [1] "private_networking"
## 
## $region$features[[2]]
## [1] "backups"
## 
## $region$features[[3]]
## [1] "ipv6"
## 
## $region$features[[4]]
## [1] "metadata"
## 
## $region$features[[5]]
## [1] "install_agent"
## 
## $region$features[[6]]
## [1] "storage"
## 
## $region$features[[7]]
## [1] "image_transfer"
## 
## 
## $region$available
## [1] TRUE
## 
## $region$sizes
## $region$sizes[[1]]
## [1] "s-1vcpu-1gb"
## 
## $region$sizes[[2]]
## [1] "512mb"
## 
## $region$sizes[[3]]
## [1] "s-1vcpu-2gb"
## 
## $region$sizes[[4]]
## [1] "1gb"
## 
## $region$sizes[[5]]
## [1] "s-3vcpu-1gb"
## 
## $region$sizes[[6]]
## [1] "s-2vcpu-2gb"
## 
## $region$sizes[[7]]
## [1] "s-1vcpu-3gb"
## 
## $region$sizes[[8]]
## [1] "s-2vcpu-4gb"
## 
## $region$sizes[[9]]
## [1] "2gb"
## 
## $region$sizes[[10]]
## [1] "s-4vcpu-8gb"
## 
## $region$sizes[[11]]
## [1] "m-1vcpu-8gb"
## 
## $region$sizes[[12]]
## [1] "c-2"
## 
## $region$sizes[[13]]
## [1] "4gb"
## 
## $region$sizes[[14]]
## [1] "c2-2vcpu-4gb"
## 
## $region$sizes[[15]]
## [1] "g-2vcpu-8gb"
## 
## $region$sizes[[16]]
## [1] "gd-2vcpu-8gb"
## 
## $region$sizes[[17]]
## [1] "m-16gb"
## 
## $region$sizes[[18]]
## [1] "s-8vcpu-16gb"
## 
## $region$sizes[[19]]
## [1] "s-6vcpu-16gb"
## 
## $region$sizes[[20]]
## [1] "c-4"
## 
## $region$sizes[[21]]
## [1] "8gb"
## 
## $region$sizes[[22]]
## [1] "c2-4vpcu-8gb"
## 
## $region$sizes[[23]]
## [1] "m-2vcpu-16gb"
## 
## $region$sizes[[24]]
## [1] "m3-2vcpu-16gb"
## 
## $region$sizes[[25]]
## [1] "g-4vcpu-16gb"
## 
## $region$sizes[[26]]
## [1] "gd-4vcpu-16gb"
## 
## $region$sizes[[27]]
## [1] "m6-2vcpu-16gb"
## 
## $region$sizes[[28]]
## [1] "m-32gb"
## 
## $region$sizes[[29]]
## [1] "s-8vcpu-32gb"
## 
## $region$sizes[[30]]
## [1] "c-8"
## 
## $region$sizes[[31]]
## [1] "16gb"
## 
## $region$sizes[[32]]
## [1] "c2-8vpcu-16gb"
## 
## $region$sizes[[33]]
## [1] "m-4vcpu-32gb"
## 
## $region$sizes[[34]]
## [1] "m3-4vcpu-32gb"
## 
## $region$sizes[[35]]
## [1] "g-8vcpu-32gb"
## 
## $region$sizes[[36]]
## [1] "s-12vcpu-48gb"
## 
## $region$sizes[[37]]
## [1] "gd-8vcpu-32gb"
## 
## $region$sizes[[38]]
## [1] "m6-4vcpu-32gb"
## 
## $region$sizes[[39]]
## [1] "m-64gb"
## 
## $region$sizes[[40]]
## [1] "s-16vcpu-64gb"
## 
## $region$sizes[[41]]
## [1] "c-16"
## 
## $region$sizes[[42]]
## [1] "32gb"
## 
## $region$sizes[[43]]
## [1] "c2-16vcpu-32gb"
## 
## $region$sizes[[44]]
## [1] "m-8vcpu-64gb"
## 
## $region$sizes[[45]]
## [1] "m3-8vcpu-64gb"
## 
## $region$sizes[[46]]
## [1] "g-16vcpu-64gb"
## 
## $region$sizes[[47]]
## [1] "s-20vcpu-96gb"
## 
## $region$sizes[[48]]
## [1] "48gb"
## 
## $region$sizes[[49]]
## [1] "gd-16vcpu-64gb"
## 
## $region$sizes[[50]]
## [1] "m6-8vcpu-64gb"
## 
## $region$sizes[[51]]
## [1] "m-128gb"
## 
## $region$sizes[[52]]
## [1] "s-24vcpu-128gb"
## 
## $region$sizes[[53]]
## [1] "c-32"
## 
## $region$sizes[[54]]
## [1] "64gb"
## 
## $region$sizes[[55]]
## [1] "c2-32vpcu-64gb"
## 
## $region$sizes[[56]]
## [1] "m-16vcpu-128gb"
## 
## $region$sizes[[57]]
## [1] "m3-16vcpu-128gb"
## 
## $region$sizes[[58]]
## [1] "g-32vcpu-128gb"
## 
## $region$sizes[[59]]
## [1] "s-32vcpu-192gb"
## 
## $region$sizes[[60]]
## [1] "gd-32vcpu-128gb"
## 
## $region$sizes[[61]]
## [1] "m-224gb"
## 
## $region$sizes[[62]]
## [1] "m6-16vcpu-128gb"
## 
## $region$sizes[[63]]
## [1] "g-40vcpu-160gb"
## 
## $region$sizes[[64]]
## [1] "gd-40vcpu-160gb"
## 
## 
## 
## $tags
## list()
## 
## $vpc_uuid
## [1] "5243aa8a-2d90-46d9-9be8-686db7a6a9bb"
## 
## attr(,"class")
## [1] "droplet"
\end{verbatim}

The most relevant information for our purpose is the public IP address, which we can extract using the following code:

\begin{verbatim}
##      ip_address       netmask      gateway    type
## 1    10.120.0.2 255.255.240.0        <nil> private
## 2 64.225.124.41 255.255.240.0 64.225.112.1  public
\end{verbatim}

The path to the application depends on the application name, but the default name is ``glm'' so that the API is located at 64.225.124.41/glm and the documentation is located at 64.225.124.41/glm/\textbf{docs}. These links are not active because the droplet is deleted after the model is run and the results are downloaded. We recommend this as the droplet costs money as long as it is running. You can delete the droplet with the \texttt{droplet\_delete} functionality. If you keep the server running, deleting is not necessary and APIs can be updated by running \texttt{do\_deploy\_glm\_api\_only}. To summarize the process, after setting up the credentials for DigitalOcean, running \texttt{do\_deploy\_glm\_api} should set up all requirements for the API.

The computational process of the API is similar as above, but the need for a ``compute site'' is now replaced by the API and the synced folder is replaced by the server. The additional parameter required is the URL to the API endpoint, which will be given to each site, which we showed above. This parameter can be set globally in the R environment using the \texttt{distribglm\_url} environmental variable, or simply running:

during an R session where, \texttt{url} is for the \texttt{plumber} API. An analyst then would set up the model as above using \texttt{api\_setup\_model}:

One additional feature of using APIs, such as RStudio Connect, would be that you can pass in authorization information to the API with credentials (e.g.~an API key) provided to authenticated users:

We do not provide a solution to authenticating the users for fitting the models on DigitalOcean.

The model then would make the same folder structure as in the synced folder implementation above. The sites would then run \texttt{api\_estimate\_model}, which would pass the model name, what site is submitting the data, and the data set (e.g.~\texttt{mydata}). Again, the data set is not shared with the API or other sites. The \texttt{api\_estimate\_model} function calls \texttt{api\_submit\_gradient}, which gets current beta estimates for the model from the API (\texttt{api\_get\_current\_beta}), estimates the gradient values for that iteration, then submits a list of the gradient values from the output of \texttt{gradient\_value}. The \texttt{api\_estimate\_model} function runs this procedure until the model is converged and then returns the output:

This function should be run at the same time other sites are submitting data, otherwise no updates will be done. If you would like to run this in an asynchronous manner, then running \texttt{api\_submit\_gradient} would be required, or simply letting \texttt{api\_estimate\_model} run and then stopping the command after a few seconds.

We currently have a \texttt{plumber} API available at https://rsconnect.biostat.jhsph.edu/distribglm and the endpoint documentation is available at https://rsconnect.biostat.jhsph.edu/distribglm/\textbf{docs}/.

\hypertarget{results}{%
\subsection{Results?}\label{results}}

\hypertarget{issues}{%
\section{Issues}\label{issues}}

Though the system may seem simple to describe, many obstacles exist. Mainly opening any system that interacts with patient data or a database (even if it were a spreadsheet) is a potential security risk which most clinical centers will not allow. Though this caution is warranted, it may be more secure than the alternative of sending estimates in other communication systems such as email. Though emailing has the upside of a human ensuring only aggregate data is transferred, it drastically increases the potential for wrong computation. For example, the \texttt{distriblm} package can have add checks on the data for missingness, quality, the sample size is equal to that of the previous iteration/model, and other issues, which may be done at varying levels at each institution.

If a user chooses the API solution, then a server is required, and though we provide an easy solution on DigitalOcean, many institutions choose to use their own systems, which may require an administrator to oversee it. This administrator is usually trained in information systems or information technology, which is likely not part of the clinical team. Thus, providing support or interaction from the clinical team to the technical personnel can be more costly than simply emailing estimates. Lastly, many institutions and research groups would like a ``handle'' on what models are being fit with their data, and thus limits on the API need to be created, which may cause other issues or limitations on teh proposed framework.

These downsides are vastly outweighed when the system gets repeated use. Thus, fitting one model one time does not generally warrant the work needed to set up this framework. One specific example is running the same model with different combinations of sites, allowing for a formal sensitivity analysis.

Additional security requirements should like be enabled on such as platform, such as random sub-sampling of the data at each site, adding noise to the vectors and matrices passed so that the sum is the same but each individual site has distorted estimates,

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{cslreferences}
\leavevmode\hypertarget{ref-akaike1998information}{}%
Akaike, H., 1998. Information theory and an extension of the maximum likelihood principle, in: Selected Papers of Hirotugu Akaike. Springer, pp. 199--213.

\leavevmode\hypertarget{ref-battey2018distributed}{}%
Battey, H., Fan, J., Liu, H., Lu, J., Zhu, Z., 2018. Distributed testing and estimation under sparse high dimensional models. Annals of statistics 46, 1352.

\leavevmode\hypertarget{ref-bonawitz2019towards}{}%
Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V., Kiddon, C., Konecny, J., Mazzocchi, S., McMahan, H.B., others, 2019. Towards federated learning at scale: System design. arXiv preprint arXiv:1902.01046.

\leavevmode\hypertarget{ref-analogsea}{}%
Chamberlain, S., Wickham, H., Chang, W., 2020. analogsea: Interface to Digital Ocean.

\leavevmode\hypertarget{ref-duan2019odal}{}%
Duan, R., Boland, M.R., Moore, J.H., Chen, Y., 2019. ODAL: A one-shot distributed algorithm to perform logistic regressions on electronic health records data from multiple clinical sites., in: PSB. World Scientific, pp. 30--41.

\leavevmode\hypertarget{ref-duncan1980approximate}{}%
Duncan, G.M., 1980. Approximate maximum likelihood estimation with data sets that exceed computer limits. matrix 2, 1.

\leavevmode\hypertarget{ref-spark}{}%
El Emam, K., Samet, S., Arbuckle, L., Tamblyn, R., Earle, C., Kantarcioglu, M., 2013. A secure distributed logistic regression protocol for the detection of rare adverse drug events. Journal of the American Medical Informatics Association 20, 453--461.

\leavevmode\hypertarget{ref-2017arXiv171207557G}{}%
Geyer, R.C., Klein, T., Nabi, M., 2017. Differentially Private Federated Learning: A Client Level Perspective. ArXiv e-prints.

\leavevmode\hypertarget{ref-ohdsi}{}%
Hripcsak, G., Duke, J.D., Shah, N.H., Reich, C.G., Huser, V., Schuemie, M.J., Suchard, M.A., Park, R.W., Wong, I.C.K., Rijnbeek, P.R., others, 2015. Observational Health Data Sciences and Informatics (OHDSI): Opportunities for observational researchers. Studies in health technology and informatics 216, 574.

\leavevmode\hypertarget{ref-jordan2019communication}{}%
Jordan, M.I., Lee, J.D., Yang, Y., 2019. Communication-efficient distributed statistical inference. Journal of the American Statistical Association 114, 668--681.

\leavevmode\hypertarget{ref-li2019federated}{}%
Li, T., Sahu, A.K., Talwalkar, A., Smith, V., 2019. Federated learning: Challenges, methods, and future directions. arXiv preprint arXiv:1908.07873.

\leavevmode\hypertarget{ref-lu2015webdisco}{}%
Lu, C.-L., Wang, S., Ji, Z., Wu, Y., Xiong, L., Jiang, X., Ohno-Machado, L., 2015. WebDISCO: A web service for distributed cox model learning without patient-level data sharing. Journal of the American Medical Informatics Association 22, 1212--1219.

\leavevmode\hypertarget{ref-mcculloch2000generalized}{}%
McCulloch, C.E., 2000. Generalized linear models. Journal of the American Statistical Association 95, 1320--1324.

\leavevmode\hypertarget{ref-mcdonald2009efficient}{}%
Mcdonald, R., Mohri, M., Silberman, N., Walker, D., Mann, G.S., 2009. Efficient large-scale distributed training of conditional maximum entropy models, in: Advances in Neural Information Processing Systems. pp. 1231--1239.

\leavevmode\hypertarget{ref-mcmahan2016communication}{}%
McMahan, H.B., Moore, E., Ramage, D., Hampson, S., others, 2016. Communication-efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629.

\leavevmode\hypertarget{ref-o2008remote}{}%
O'Keefe, C.M., Good, N.M., 2008. A remote analysis server-what does regression output look like?, in: International Conference on Privacy in Statistical Databases. Springer, pp. 270--283.

\leavevmode\hypertarget{ref-o2009regression}{}%
O'Keefe, C.M., Good, N.M., 2009. Regression output from a remote analysis server. Data \& Knowledge Engineering 68, 1175--1186.

\leavevmode\hypertarget{ref-RCORE}{}%
R Core Team, 2020. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.

\leavevmode\hypertarget{ref-corda}{}%
Rehak, D., Dodds, P., Lannom, L., 2005. A model and infrastructure for federated learning content repositories, in: Interoperability of Web-Based Educational Systems Workshop. Citeseer.

\leavevmode\hypertarget{ref-omop}{}%
Stang, P.E., Ryan, P.B., Racoosin, J.A., Overhage, J.M., Hartzema, A.G., Reich, C., Welebob, E., Scarnecchia, T., Woodcock, J., 2010. Advancing the science for active surveillance: Rationale and design for the Observational Medical Outcomes Partnership. Annals of internal medicine 153, 600--606.

\leavevmode\hypertarget{ref-tong2020robust}{}%
Tong, J., Duan, R., Li, R., Scheuemie, M.J., Moore, J.H., Chen, Y., 2020. Robust-ODAL: Learning from heterogeneous health systems without sharing patient-level data, in: Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing. World Scientific, p. 695.

\leavevmode\hypertarget{ref-plumber}{}%
Trestle Technology, LLC, 2018. plumber: An API generator for R.

\leavevmode\hypertarget{ref-wang2017efficient}{}%
Wang, J., Kolar, M., Srebro, N., Zhang, T., 2017. Efficient distributed learning with sparsity, in: Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, pp. 3636--3645.

\leavevmode\hypertarget{ref-explorer}{}%
Wang, S., Jiang, X., Wu, Y., Cui, L., Cheng, S., Ohno-Machado, L., 2013. EXpectation propagation LOgistic REgRession (EXPLORER): Distributed privacy-preserving online model learning. Journal of biomedical informatics 46, 480--496.

\leavevmode\hypertarget{ref-glore}{}%
Wu, Y., Jiang, X., Kim, J., Ohno-Machado, L., 2012. G rid binary LO gistic RE gression (GLORE): Building shared models without sharing data. Journal of the American Medical Informatics Association 19, 758--764.

\leavevmode\hypertarget{ref-zhang2013communication}{}%
Zhang, Y., Duchi, J.C., Wainwright, M.J., 2013. Communication-efficient algorithms for statistical optimization. The Journal of Machine Learning Research 14, 3321--3363.

\leavevmode\hypertarget{ref-zinkevich2010parallelized}{}%
Zinkevich, M., Weimer, M., Li, L., Smola, A.J., 2010. Parallelized stochastic gradient descent, in: Advances in Neural Information Processing Systems. pp. 2595--2603.
\end{cslreferences}


\end{document}

